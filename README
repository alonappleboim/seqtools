This script performs everything needed to transform a collection of
fastq files generated by the transeq protocol to many (optional)
different outputs.

The implementation tries to use parallelization and efficient implementations
(i.e. heavylifting is not pythonic) as much as possible. The basic idea is to
split the work to samples, which are mostly independent, and collect all
results at the end.


The workflow is:

 splitting barcodes
    First pass with awk to make a file for every exact barcode match in the data.
    Quick and (very) dirty.
    fastq(s).gz -> fastq (per existing barcode + 1 for unknowns)
    (print total # reads)

 At this point the pipeline generates a managing process per sample with sbatch, and dies. When they all finish,
 its cousin ("transeq_post_processing") is activated and performs required merges and cleanups.

splitting barcodes (2)
    Second pass - going over the barcodes the awk script didn't recognize and taking the
    relevant ones (i.e. upto hamming distance d from scripts barcode).
    passing this file to the next step


 aligning to genome
    fastq (batched) -> sorted bam (batched)
    QC: alignment statistics
        no alignment reads in bam format (optional)
        no alignment reads in bam format (optional)

 merging
    sorted bam (batched) -> sorted bam (per sample)

 removing duplicates
    sorted bam -> sorted bam
    QC: duplicate counts
        duplicate examples in bam format (optional)

 building coverage/start/end maps
    sorted bam -> bed, bigwig
    QC: length distribution

 building a track hub
    bed -> bigwig, hub directory

# analysis, another program
 peak calling
    sorted bam -> bed, csv
    QC: peak counts
        peak start/end/coverage averages

 annotation meta (for a given annotation, average signals in certain window)
    sorted bam, bed ->

 annotation counting
    sorted bam, bed -> csv
    QC: annotation counts
        annotation start/end/coverage averages


"""
