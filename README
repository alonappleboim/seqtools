This script performs everything needed to transform a collection of
fastq files generated by the transeq protocol to many (optional)
different outputs.

Examples:
========

basic:
    python /cs/bd/TranSEQ/transeq_pipeline/run.py -fp /my/fastq/path/

with k.lac alignemnt:
    python /cs/bd/TranSEQ/transeq_pipeline/run.py -fp /my/fastq/path/ -kip /cs/wetlab/genomics/klac/bowtie/genome

debugging pipeline (recommended before long runs, in this case runs on only 1e5 reads and 10 samples):
    python ~/Dropbox/workspace/transeq_pipeline/run.py -fp /my/fastq/path/ -d 100000,10

reporting stuff to your email:
    python ~/Dropbox/workspace/transeq_pipeline/run.py -fp /my/fastq/path/ -ue email1@some.domain,email2@same.domain

saving results to a specified path:
    python ~/Dropbox/workspace/transeq_pipeline/run.py -fp /my/fastq/path/ -ep ~/Dropbox/transeq_stats/

keeping unaligned reads:
    python ~/Dropbox/workspace/transeq_pipeline/run.py -fp /my/fastq/path/ -ku

keeping filteres, unaligned, and no-barcode reads:
    python ~/Dropbox/workspace/transeq_pipeline/run.py -fp /my/fastq/path/ -kf -ku -knb

restarting from fastq stage - TODO: CURRENTLY Not working!
    python ~/Dropbox/workspace/transeq_pipeline/run.py -fp /previous/transeq/results/path/ -sa FASTQ


More Info:
=========

The implementation tries to use parallelization and efficient implementations
(i.e. heavylifting is not pythonic) as much as possible. The basic idea is to
split the work to samples, which are (mostly) independent, and collect all
results at the end.


The workflow is:
 parsing inputs and pre-processing
    performing all parsing tasks (samples, features, filters, exporters, etc), followed by all
    pre-processing steps - generating folders, making sure executables exist etc.

 splitting barcodes
    First pass with awk to make a file for every exact barcode match in the data.
    Quick and (very) dirty.
    Second pass again with awk to collect sequences with upto a specificd hamming distance from barcodes.
    The output is written to the temp_dir and is not in fastq format.
    There's an optional output file for no-barcode reads.

 Writing fastq files
    Next, a process is generated per sample, that transforms the previous stage output into regular gzipped fastq
    files.

 Alignment
    Each sample aligns reads to genome in an independent process. If a klac genome is specified, reads are also
    aligned to that genome (assumed to be klac) #TODO: change this to an arbitrary genome
    Only aligned reads are further reported, an optional "unaligend" folder can be generated to hold all unaligned
    reads for further inspection.

 Filtering
    The bam output from the alignment is filtered, and only those reads that pass the all the filters are written
    to the BAM folder. an optional "filtered" folder can be generated that holds all filtered reads in BAM format.
    The reads that passed filtration are now sorted and indexed .bam files in the BAM folder.

 Track generation
    Each bam file generates 2 bigwig files in the BIGWIG folder, one per strand (.c.bw, and .w.bw)

 Counting
    For the given annotation file, and specified window, reads are counted and reported per annotation per sample.

 Exporting
    Each specified exporter exports the overall statistics and counts in a specific format to optional specific
    locations
